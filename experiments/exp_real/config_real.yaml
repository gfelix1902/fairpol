#Configuration for simulation
data:
  dataset: "real"
  train_frac: 0.7
  val_frac: 0.1
  y_type: "continuous"

#Configuration for experiment
experiment:
  runs: 5
  seed: 12345
  plotting: False                #Plot representations
  neptune: True                 #Logging of training/ validation loss
  validation: True                  #True if validation loss should be computed
  hyper_path: "/hyperparam/exp_real"    #Path of hyperparameters
  oracle_nuisance: False                 #True if ground-truth nuisance parameters should be used (simualted data)
  nuisance_only: False                      #True if no policy should be learned, only representation
  save_results: True                    #If True, results will be stored in /results folder
  m: "dr"                              #Score used to estimate policy value: dm = direct, ipw = inverse propensity weighted, dr = doubly robust
  models:                                 #Models to be trained and evaluated: ours: fpnet
    - name: "fpnet"
      action_fair: "auf"                 #Action fairness: af_conf = domain confusion loss, af_wstein = wasserstein loss, af_gr = gradient reversal
      value_fair: "vuf"
    - name: "fpnet"
      action_fair: "af_conf"                 #Action fairness: af_conf = domain confusion loss, af_wstein = wasserstein loss, af_gr = gradient reversal
      value_fair: "vuf"                       #Value fairness: vuf = value unfair, vmm: max-min fair, vef: envy-free
    - name: "fpnet"
      action_fair: "af_conf"
      value_fair: "vmm"
    - name: "fpnet"
      action_fair: "af_conf"
      value_fair: "vef"
    - name: "fpnet"
      action_fair: "auf"
      value_fair: "vef"





